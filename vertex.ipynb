{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "REGION = os.getenv('REGION')\n",
    "BUCKET_NAME = os.getenv('BUCKET_NAME')\n",
    "PIPELINE_JSON_PATH = os.getenv('PIPELINE_JSON_PATH')\n",
    "#SERVICE_ACCOUNT = os.getenv('SERVICE_ACCOUNT')\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    #service_account=SERVICE_ACCOUNT\n",
    ")\n",
    "print(\"Vertex AI initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Initialize a storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Specify your bucket name\n",
    "bucket_name = BUCKET_NAME\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "# Check if you can access the bucket\n",
    "if bucket.exists():\n",
    "    print(f\"Access to bucket '{bucket_name}' verified successfully.\")\n",
    "else:\n",
    "    print(f\"Cannot access bucket '{bucket_name}'. Check permissions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stell\\AppData\\Local\\Temp\\ipykernel_14724\\2726544330.py:1: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import dsl\n",
      "c:\\Users\\stell\\.conda\\envs\\cloud\\Lib\\site-packages\\kfp\\dsl\\component_decorator.py:126: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.9' to 'python:3.10' on Oct 1, 2025. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.10.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component, Output, Input, Dataset, Model\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data extraction from BigQuery\n",
    "@component(\n",
    "    base_image=\"python:3.9\",  # Specify a base image with Python 3.9\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\"]\n",
    ")\n",
    "def extract_data_from_bigquery(\n",
    "    query: str,\n",
    "    output_data_path: Output[Dataset]  # Output to save the file\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Fetch data\n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    df.to_csv(output_data_path.path, index=False)\n",
    "    print(f\"Data extracted and saved to {output_data_path.path}\")\n",
    "\n",
    "# Step 2: Preprocessing data\n",
    "@component\n",
    "def preprocess_data(\n",
    "    input_data_path: Input[Dataset],  # Input path from the extraction task\n",
    "    target_column: str,\n",
    "    train_data_path: Output[Dataset],  # Output paths for training and testing data\n",
    "    test_data_path: Output[Dataset]\n",
    "):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(input_data_path.path)\n",
    "\n",
    "    # Preprocessing steps (fill missing values, encode, etc.)\n",
    "    df = df.fillna(0)\n",
    "    df['state'] = df['state'].astype('category').cat.codes  # Encoding categorical variables\n",
    "\n",
    "    # Split into features and target\n",
    "    features = df.drop(columns=[target_column])\n",
    "    target = df[target_column]\n",
    "\n",
    "    # Split into train and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Save the processed data to output paths\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_data.to_csv(train_data_path.path, index=False)\n",
    "    test_data.to_csv(test_data_path.path, index=False)\n",
    "    print(f\"Data processed and saved to {train_data_path.path} and {test_data_path.path}\")\n",
    "\n",
    "# Step 3: Train the regression model\n",
    "@component\n",
    "def train_model(\n",
    "    train_data_path: Input[Dataset],  # Input training data\n",
    "    model_output_path: Output[Model]  # Output path to save the model\n",
    "):\n",
    "    # Load the training data\n",
    "    train_data = pd.read_csv(train_data_path.path)\n",
    "\n",
    "    # Assume 'hospitalized' is the target\n",
    "    y_train = train_data.pop('hospitalized')\n",
    "\n",
    "    # Train the model using XGBoost (Regressor if it's a regression problem)\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(train_data, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_model(model_output_path.path)\n",
    "    print(f\"Model trained and saved to {model_output_path.path}\")\n",
    "\n",
    "# Step 4: Evaluate the model's performance\n",
    "@component\n",
    "def evaluate_model(\n",
    "    model_path: Input[Model],  # Input trained model\n",
    "    test_data_path: Input[Dataset]  # Input test data\n",
    "):\n",
    "    # Load the model and test data\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.load_model(model_path.path)\n",
    "\n",
    "    test_data = pd.read_csv(test_data_path.path)\n",
    "\n",
    "    # Separate features and target\n",
    "    y_test = test_data.pop('hospitalized')\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(test_data)\n",
    "    accuracy = accuracy_score(y_test, y_pred.round())  # If regression, round predictions for accuracy\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Define the pipeline\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"mlcov_pipeline\",\n",
    "    description=\"A pipeline to train and evaluate a regression model\",\n",
    "    pipeline_root=BUCKET_NAME\n",
    ")\n",
    "def regression_pipeline(\n",
    "    bq_query: str = \"SELECT * FROM `data-id`\",\n",
    "    target_column: str = \"hospitalized\"\n",
    "):\n",
    "    # Step 1: Extract data from BigQuery\n",
    "    extract_task = extract_data_from_bigquery(query=bq_query)\n",
    "\n",
    "    # Step 2: Preprocess data\n",
    "    preprocess_task = preprocess_data(\n",
    "        input_data_path=extract_task.outputs['output_data_path'],\n",
    "        target_column=target_column\n",
    "    )\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    train_task = train_model(\n",
    "        train_data_path=preprocess_task.outputs['train_data_path']\n",
    "    )\n",
    "\n",
    "    # Step 4: Evaluate the model\n",
    "    evaluate_task = evaluate_model(\n",
    "        model_path=train_task.outputs['model_output_path'],\n",
    "        test_data_path=preprocess_task.outputs['test_data_path']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# # Compile the pipeline to a JSON file\n",
    "# pipeline_func = regression_pipeline\n",
    "# compiler.Compiler().compile(pipeline_func=pipeline_func, package_path='mlcov_reg2.json')\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=regression_pipeline,\n",
    "    package_path=\"mlcov_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AI Platform\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,  # Replace <YOUR_REGION> with your region, e.g., \"us-central1\"\n",
    ")\n",
    "\n",
    "# Define the job specification\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"mlcov_inc_pipeline\",\n",
    "    template_path=PIPELINE_JSON_PATH,  # Path to the job spec in Cloud Storage\n",
    "    pipeline_root= BUCKET_NAME,  # Where to store pipeline artifacts\n",
    "    parameter_values={\n",
    "        \"bq_query\": \"SELECT * FROM `data-id`\",\n",
    "        \"target_column\": \"hospitalized\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "job.run(sync=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
